{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heinsense2/AIO_CaseStudy/blob/main/Training_on_FathomNet_Custom_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpTpijONjCPL"
      },
      "source": [
        "## Custom training using YOLOv5 on Fathomnet custom dataset\n",
        "\n",
        "This notebook explains how to train a custom dataset using YOLOv5 to recognize different marine species presnt in the Monterey bay. This notebook serves as a guideline to produce the results presented in the paper\n",
        "\n",
        " *Demystifying image-based machine learning: a practical guide to automated analysis of imagery using modern machine learning tools*, \n",
        "\n",
        "\n",
        "The data is prepared using code available [here](https://github.com/heinsense2/AIO_CaseStudy/tree/main/data/scripts).\n",
        "\n",
        "NOTE: If you wish to use this notebook, you will need to make changes to refer to the datast locations and python script parameters amoung others.\n",
        "\n",
        "Here are the relevant steps:\n",
        "\n",
        "*   Create the dataset and annotations (labels). Organize directories.\n",
        "*   Export dataset to YOLOv5\n",
        "*   Train YOLOv5 to recognize the objects (marine animals) in our dataset\n",
        "*   Evaluate our YOLOv5 model's performance\n",
        "*   Run inference to view the model at work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uruljSVEk_hc"
      },
      "source": [
        "# 1. Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaA_HZi8lK4U"
      },
      "outputs": [],
      "source": [
        "# Clone YOLOv5\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install torch==1.8.1 torchvision==0.9.1\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXnRWZ2nnQuD"
      },
      "source": [
        "# 2. Assemble Dataset\n",
        "\n",
        "To train our model, we need to assemble a dataset of representative images with bounding boxes around the objects we want to detect. Our dataset must be in YOLOv5 format.\n",
        "\n",
        "The Fathomnet data is downloaded and prepared using code available [here](https://github.com/heinsense2/AIO_CaseStudy/tree/main/data/scripts).\n",
        "\n",
        "\n",
        "When usig Google Colab, it is recommended to have the data available on Google Drive. So we need to first mount our Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vncfn_fqoeLW",
        "outputId": "694e27ba-2f11-4c7f-f513-ef4b1260dbf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07QSJM5iuvar",
        "outputId": "615ded60-3249-40f7-82dc-ed3060f70501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset45.yaml\timages\tlabels\n"
          ]
        }
      ],
      "source": [
        "# List the directory where the data resides\n",
        "#!ls \"/content/gdrive/My Drive/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# 3. Train Our Custom YOLOv5 model\n",
        "\n",
        "We are able to pass a number of arguments, here is what we used:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `data.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQttoa-D2mgn"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 640 --batch 16 --epochs 10 --data {data.directory}/{domain}.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "All results are logged by default to runs/train, with a new experiment directory created for each new training (runs/train/exp2, runs/train/exp3)\n",
        "\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a CSV logfile results.csv\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs/train/exp*\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also validate the trained detection model on the test dataset by using the val.py script in YOLOv5.\n"
      ],
      "metadata": {
        "id": "ffwecYhRKF8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python val.py --data {data.directory}/{domain}.yaml --weights runs/train/exp/weights/best.pt --task test"
      ],
      "metadata": {
        "id": "o-_FjSzJKqlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkJrK89z_O0q"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 640 --conf 0.65 --source {dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9h22JUt_m56"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.png'): #assuming PNG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Trained Weights for Future Inference\n",
        "You can now export the trained weights from our detector for inference on your device elsewhere.\n"
      ],
      "metadata": {
        "id": "KAXD9i3I91CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')"
      ],
      "metadata": {
        "id": "yfgdDBYC-ztS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}